--- File Tree Structure ---
|-- data/
|-- benchmark.go
|-- check-data.go
|-- common.go
|-- data.go
|-- main.go
|-- math.go
|-- metrics.go
|-- test.go
    |-- BTCUSDT/
    |-- DOGEUSDT/
    |-- ETHUSDT/
    |-- SOLUSDT/
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [07..12]/ (6 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [09..12]/ (4 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)

// --- File: benchmark.go ---

```go
package main

import (
	"fmt"
	"math"
	"math/rand/v2"
	"os"
	"runtime"
	"runtime/debug"
	"sort"
	"sync"
	"sync/atomic"
	"text/tabwriter"
	"time"
	"unsafe"
)

type threadContext struct {
	// Infrastructure
	ws   *MathWorkspace
	cols *DayColumns

	// Math Engine
	gens []Generator
	sigs [][]float64 // Output buffer: [FeatureIdx][Row]

	// Explicit padding to reduce False Sharing between hot per-thread states.
	// Sized so that each context occupies at least ~128 bytes on 64-bit targets.
	_ [120]byte
}

func runBenchmark() {
	cores := runtime.GOMAXPROCS(0)

	// 1. Setup Dynamic Math Engine
	coreGens := GetCorePrimitives()
	featureCount := len(coreGens)
	featureNames := make([]string, featureCount)
	for i, g := range coreGens {
		featureNames[i] = g.Name()
	}

	fmt.Println("\n>>> QUANT-GRADE STRESS TEST (MODULAR ENGINE) <<<")
	fmt.Printf("    Hardware: %d Threads | %s / %s | Zen 4 Optimization: ACTIVE\n", cores, runtime.GOOS, runtime.GOARCH)
	fmt.Printf("    Target  : 7 Core Primitives (Stateful) + Zero-Alloc Context Bridge\n")
	fmt.Printf("    Signals : %v\n\n", featureNames)

	f, err := os.Create("Benchmark_Elite_Report.txt")
	if err != nil {
		fmt.Printf("[fatal] cannot create benchmark report: %v\n", err)
		return
	}
	defer f.Close()

	// 2. DATA GENERATION
	rows := 1_000_000
	fmt.Printf(" [1/5] Generating Synthetic Data (%d rows)... ", rows)

	// We use the Pool to get a clean structure, then populate it
	benchCols := DayColumnPool.Get().(*DayColumns)
	generateSyntheticColumns(benchCols, rows)

	fmt.Println("Done.")

	// 3. LATENCY DISTRIBUTION (Jitter Test)
	fmt.Println(" [2/5] Measuring Tail Latency (Jitter)...")
	warmup := 5
	samples := 50
	latencies := make([]time.Duration, samples)

	// Pre-allocate single thread context for latency test
	latencyCtx := &threadContext{
		ws:   &MathWorkspace{},
		cols: benchCols, // Share the read-only data
		gens: GetCorePrimitives(),
		sigs: make([][]float64, featureCount),
	}
	for i := range latencyCtx.sigs {
		latencyCtx.sigs[i] = make([]float64, rows)
	}

	debug.SetGCPercent(-1)
	runtime.GC()
	debug.SetGCPercent(200)

	for i := 0; i < warmup+samples; i++ {
		start := time.Now()

		// --- HOT PATH BEGIN ---
		// 1. The Bridge (Raw -> Fluid)
		mathCtx := PrepareMathContext(latencyCtx.cols, latencyCtx.ws)

		// 2. The Engine
		for sIdx, gen := range latencyCtx.gens {
			gen.Update(mathCtx, latencyCtx.sigs[sIdx])
		}
		// --- HOT PATH END ---

		dur := time.Since(start)
		if i >= warmup {
			latencies[i-warmup] = dur
		}

		// Reset generators for next run to ensure identical compute load
		for _, g := range latencyCtx.gens {
			g.Reset()
		}
	}

	sort.Slice(latencies, func(i, j int) bool { return latencies[i] < latencies[j] })
	p50 := latencies[samples/2]
	p99Idx := int(math.Ceil(0.99*float64(samples))) - 1
	if p99Idx >= samples {
		p99Idx = samples - 1
	}
	p99 := latencies[p99Idx]

	// 4. THROUGHPUT & BANDWIDTH
	fmt.Println(" [3/5] Measuring Memory Bandwidth...")

	// Input: Time(8)+Price(8)+Qty(8)+Side(1)+Match(2) = 27 bytes/row
	// Context: DT(8)+DP(8)+LogRet(8)+RawTCI(8) = 32 bytes/row (Derived)
	// Output: 7 signals * 8 bytes = 56 bytes/row
	// Total RW traffic per pass approx: (27 reads) + (32 writes/reads) + (56 writes)
	// Realistically, we measure "Effective" throughput based on input size.
	inputBytes := uint64(rows * 27)
	outputBytes := uint64(rows * featureCount * 8)
	totalBytes := inputBytes + outputBytes

	minDur := latencies[0]
	if minDur == 0 {
		minDur = 1
	}
	gbPerSec := (float64(totalBytes) / minDur.Seconds()) / 1024 / 1024 / 1024

	// 5. GC PAUSE ANALYSIS
	fmt.Println(" [4/5] Measuring Garbage Collector 'Stop-The-World' Pauses...")
	var gcStats debug.GCStats
	// Force some allocations to trigger GC if any exist (though we aim for zero)
	for i := 0; i < 20; i++ {
		mathCtx := PrepareMathContext(latencyCtx.cols, latencyCtx.ws)
		for sIdx, gen := range latencyCtx.gens {
			gen.Update(mathCtx, latencyCtx.sigs[sIdx])
		}
	}
	debug.ReadGCStats(&gcStats)
	pauseTotal := gcStats.PauseTotal
	numGC := gcStats.NumGC
	var maxPause time.Duration
	for _, p := range gcStats.Pause {
		if p > maxPause {
			maxPause = p
		}
	}

	// 6. SCALABILITY (Saturation)
	fmt.Println(" [5/5] Torture Test: All Cores Saturation (5 Seconds)...")

	// Prepare threaded contexts
	contexts := make([]*threadContext, cores)
	for i := 0; i < cores; i++ {
		ctx := &threadContext{
			ws:   &MathWorkspace{},
			cols: benchCols, // They all read the same input memory (good for cache test)
			gens: GetCorePrimitives(),
			sigs: make([][]float64, featureCount),
		}
		for k := range ctx.sigs {
			ctx.sigs[k] = make([]float64, rows)
		}
		contexts[i] = ctx
	}

	var wg sync.WaitGroup
	var totalBatches atomic.Int64

	runtime.GC()
	stopChan := make(chan struct{})

	for i := 0; i < cores; i++ {
		wg.Add(1)
		ctx := contexts[i]
		go func() {
			defer wg.Done()
			// Localize pointers to avoid pointer chasing in loop
			myGens := ctx.gens
			mySigs := ctx.sigs
			myCols := ctx.cols
			myWs := ctx.ws

			for {
				select {
				case <-stopChan:
					return
				default:
					// Benchmark the full pipeline
					mathCtx := PrepareMathContext(myCols, myWs)
					for sIdx, gen := range myGens {
						gen.Update(mathCtx, mySigs[sIdx])
					}

					// We don't reset generators inside the hot loop to simulate "streaming"
					// where state persists, effectively making the test strictly about compute/memory.

					totalBatches.Add(1)
				}
			}
		}()
	}

	startMulti := time.Now()
	time.Sleep(5 * time.Second)
	close(stopChan)
	wg.Wait()
	durMulti := time.Since(startMulti)

	batchesDone := totalBatches.Load()
	totalEvents := int64(rows) * batchesDone
	throughputMulti := float64(totalEvents) / durMulti.Seconds()

	// Approx FLOPS estimation:
	// Each row involves exp(), log(), divs, accumulation.
	// ~50 FLOPs per primitive per row * 7 primitives = 350 FLOPS/row
	flops := throughputMulti * 350.0 / 1_000_000_000.0

	printEliteReport(os.Stdout, p50, p99, gbPerSec, throughputMulti, cores, rows, numGC, maxPause, pauseTotal, flops)
	printEliteReport(f, p50, p99, gbPerSec, throughputMulti, cores, rows, numGC, maxPause, pauseTotal, flops)

	// Cleanup (optional since program ends)
	DayColumnPool.Put(benchCols)
}

func printEliteReport(out *os.File, p50, p99 time.Duration, gbps, tput float64, cores, rows int, numGC int64, maxPause, totalPause time.Duration, flops float64) {
	w := tabwriter.NewWriter(out, 0, 0, 2, ' ', 0)
	fmt.Fprintln(w, "\n==========================================================")
	fmt.Fprintln(w, "             QUANTITATIVE PERFORMANCE METRICS             ")
	fmt.Fprintln(w, "==========================================================")
	fmt.Fprintf(w, "    Target Batch    : %d rows\n", rows)

	fmt.Fprintln(w, "\n1. COMPUTE POWER")
	fmt.Fprintf(w, "    Core Util       : %d Threads\n", cores)
	fmt.Fprintf(w, "    Throughput      : %.0f rows/sec\n", tput)
	fmt.Fprintf(w, "    Est. FLOPS      : %.2f GFLOPS\n", flops)
	fmt.Fprintf(w, "    Daily Speed     : ~%.0f days/sec\n", tput/86400.0)

	fmt.Fprintln(w, "\n2. LATENCY & MEMORY")
	fmt.Fprintf(w, "    p50 Latency     : %v\n", p50)
	fmt.Fprintf(w, "    p99 Latency     : %v\n", p99)
	fmt.Fprintf(w, "    Effective B/W   : %.2f GB/s\n", gbps)

	fmt.Fprintln(w, "\n3. GARBAGE COLLECTION")
	fmt.Fprintf(w, "    Num GCs         : %d\n", numGC)
	fmt.Fprintf(w, "    Pause Total     : %v\n", totalPause)
	fmt.Fprintf(w, "    Max Pause       : %v\n", maxPause)
	fmt.Fprintln(w, "==========================================================")
	w.Flush()
}

func generateSyntheticColumns(cols *DayColumns, rows int) {
	cols.Reset()

	// Ensure capacity
	if cap(cols.Times) < rows {
		cols.Times = make([]int64, rows)
		cols.Prices = make([]float64, rows)
		cols.Qtys = make([]float64, rows)
		cols.Sides = make([]int8, rows)
		cols.Matches = make([]uint16, rows)
	}
	cols.Times = cols.Times[:rows]
	cols.Prices = cols.Prices[:rows]
	cols.Qtys = cols.Qtys[:rows]
	cols.Sides = cols.Sides[:rows]
	cols.Matches = cols.Matches[:rows]

	rng := rand.New(rand.NewPCG(uint64(time.Now().UnixNano()), 999))

	baseTime := int64(1704067200000)
	basePrice := 50000.0

	// Unsafe Pointers for fast generation
	pTimes := unsafe.SliceData(cols.Times)
	pPrices := unsafe.SliceData(cols.Prices)
	pQtys := unsafe.SliceData(cols.Qtys)
	pSides := unsafe.SliceData(cols.Sides)
	pMatches := unsafe.SliceData(cols.Matches)

	for i := 0; i < rows; i++ {
		// Time increases by 0 to 20ms
		baseTime += int64(rng.Uint64() % 20)
		*(*int64)(unsafe.Pointer(uintptr(unsafe.Pointer(pTimes)) + uintptr(i)*8)) = baseTime

		// Random walk price
		if i > 0 {
			basePrice += (rng.Float64() - 0.5) * 10.0
		}
		*(*float64)(unsafe.Pointer(uintptr(unsafe.Pointer(pPrices)) + uintptr(i)*8)) = basePrice

		// Random Qty
		*(*float64)(unsafe.Pointer(uintptr(unsafe.Pointer(pQtys)) + uintptr(i)*8)) = rng.Float64() * 2.0

		// Random Side
		s := int8(1)
		if rng.Uint64()&1 == 0 {
			s = -1
		}
		*(*int8)(unsafe.Pointer(uintptr(unsafe.Pointer(pSides)) + uintptr(i)*1)) = s

		// Random matches
		*(*uint16)(unsafe.Pointer(uintptr(unsafe.Pointer(pMatches)) + uintptr(i)*2)) = 1
	}
	cols.Count = rows
}
```

// --- End File: benchmark.go ---

// --- File: check-data.go ---

```go
package main

import (
	"encoding/binary"
	"fmt"
	"io"
	"io/fs"
	"math"
	"os"
	"path/filepath"
	"sort"
	"text/tabwriter"
	"time"
)

const (
	GapThresholdSmall = 1 * time.Second
	GapThresholdLarge = 60 * time.Second
)

type FileStats struct {
	Date          string
	Rows          int
	GapCountSmall int
	GapCountLarge int
	MaxGap        time.Duration
	MinPrice      float64
	MaxPrice      float64
	ZeroQtys      int
	SizeMB        float64
	Status        string
}

func runCheck() {
	fmt.Println(">>> DATA FORENSICS REPORT | AUTO-DETECTED SYMBOLS <<<")
	fmt.Printf("    Checking for gaps > %v and integrity issues...\n\n", GapThresholdLarge)

	var symbols []string
	for sym := range discoverSymbols() {
		symbols = append(symbols, sym)
	}
	if len(symbols) == 0 {
		fmt.Printf("[fatal] No symbols found in %s\n", BaseDir)
		return
	}
	sort.Strings(symbols)

	readBuf := make([]byte, 32*1024*1024)

	for idx, sym := range symbols {
		if idx > 0 {
			fmt.Println()
		}
		runCheckForSymbol(sym, &readBuf)
	}
}

func runCheckForSymbol(sym string, readBuf *[]byte) {
	fmt.Printf(">>> DATA FORENSICS REPORT | Symbol: %s <<<\n", sym)
	fmt.Printf("    Checking for gaps > %v and integrity issues...\n\n", GapThresholdLarge)

	root := filepath.Join(BaseDir, sym)
	var files []string

	err := filepath.WalkDir(root, func(path string, d fs.DirEntry, err error) error {
		if err != nil {
			return err
		}
		if d.IsDir() {
			return nil
		}
		if d.Name() == "data.quantdev" {
			files = append(files, path)
		}
		return nil
	})
	if err != nil {
		fmt.Printf("[fatal] Error walking directory for %s: %v\n", sym, err)
		return
	}

	if len(files) == 0 {
		fmt.Printf("[warn] No data.quantdev files found for %s in %s\n", sym, root)
		return
	}
	sort.Strings(files)

	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)
	fmt.Fprintln(w, "DATE\tROWS\tSIZE(MB)\tMIN_P\tMAX_P\tGAPS(>1s)\tGAPS(>60s)\tMAX_GAP\tSTATUS")
	fmt.Fprintln(w, "----\t----\t--------\t-----\t-----\t---------\t----------\t-------\t------")

	var totalRows int64
	var totalFiles int
	var issuesFound int

	buf := *readBuf

	for _, fPath := range files {
		stats, newBuf := checkFile(fPath, buf)
		buf = newBuf
		totalRows += int64(stats.Rows)
		totalFiles++

		status := "OK"
		if stats.Status != "" {
			status = stats.Status
			issuesFound++
		} else if stats.GapCountLarge > 0 {
			status = "WARN:Gaps"
			issuesFound++
		} else if stats.Rows == 0 {
			status = "EMPTY"
		}

		minP := stats.MinPrice
		maxP := stats.MaxPrice
		if stats.Rows == 0 || stats.MinPrice == math.MaxFloat64 {
			minP, maxP = 0, 0
		}

		fmt.Fprintf(w, "%s\t%d\t%.2f\t%.2f\t%.2f\t%d\t%d\t%s\t%s\n",
			stats.Date, stats.Rows, stats.SizeMB, minP, maxP,
			stats.GapCountSmall, stats.GapCountLarge, stats.MaxGap.Round(time.Millisecond), status)
	}

	w.Flush()
	fmt.Printf("\nSUMMARY(%s): Scanned %d files | %d total rows | %d files with issues/warnings\n",
		sym, totalFiles, totalRows, issuesFound)

	*readBuf = buf
}

func checkFile(path string, buf []byte) (FileStats, []byte) {
	dir := filepath.Dir(path)
	monthDir := filepath.Base(dir)
	yearDir := filepath.Base(filepath.Dir(dir))

	stats := FileStats{
		Date:     fmt.Sprintf("%s-%s", yearDir, monthDir),
		MinPrice: math.MaxFloat64,
		MaxPrice: 0,
	}

	f, err := os.Open(path)
	if err != nil {
		stats.Status = "ERR:Open"
		return stats, buf
	}
	defer f.Close()

	var fileSize int64
	if fi, err := f.Stat(); err == nil {
		fileSize = fi.Size()
		stats.SizeMB = float64(fi.Size()) / 1024.0 / 1024.0
	}

	colsAny := DayColumnPool.Get()
	cols := colsAny.(*DayColumns)
	defer DayColumnPool.Put(cols)

	idxPath := filepath.Join(dir, "index.quantdev")
	fIdx, err := os.Open(idxPath)
	if err != nil {
		stats.Status = "ERR:NoIndex"
		return stats, buf
	}
	defer fIdx.Close()

	var header [16]byte
	if _, err := io.ReadFull(fIdx, header[:]); err != nil {
		stats.Status = "ERR:IdxHead"
		return stats, buf
	}
	if string(header[0:4]) != IdxMagic {
		stats.Status = "ERR:IdxMagic"
		return stats, buf
	}
	rawCount := binary.LittleEndian.Uint64(header[8:])
	if rawCount > uint64(math.MaxInt64) {
		stats.Status = "ERR:IdxCount"
		return stats, buf
	}
	count := int64(rawCount)

	const rowSize = 26
	var row [rowSize]byte

	for i := int64(0); i < count; i++ {
		if _, err := io.ReadFull(fIdx, row[:]); err != nil {
			stats.Status = "ERR:IdxRow"
			break
		}

		blobOffset := int64(binary.LittleEndian.Uint64(row[2:]))
		blobLen := int64(binary.LittleEndian.Uint64(row[10:]))

		if blobLen <= 0 {
			continue
		}

		// Extra safety: ensure the blob range fits within the data file.
		if fileSize > 0 {
			if blobOffset < 0 || blobLen < 0 || blobOffset > fileSize-blobLen {
				stats.Status = "ERR:IdxRange"
				continue
			}
		}

		if int64(len(buf)) < blobLen {
			buf = make([]byte, blobLen)
		}

		if _, err := f.Seek(blobOffset, io.SeekStart); err != nil {
			stats.Status = "ERR:Seek"
			continue
		}

		n, err := io.ReadFull(f, buf[:blobLen])
		if err != nil || int64(n) != blobLen {
			stats.Status = "ERR:ReadBlob"
			continue
		}

		cols.Reset()
		rowCount, ok := inflateGNCToColumns(buf[:blobLen], cols)
		if !ok {
			stats.Status = "ERR:Corrupt"
			continue
		}

		stats.Rows += rowCount
		if rowCount > 0 {
			analyzeColumns(cols, &stats)
		}
	}

	return stats, buf
}

func analyzeColumns(cols *DayColumns, stats *FileStats) {
	times := cols.Times
	prices := cols.Prices
	qtys := cols.Qtys

	for i, p := range prices {
		if p < stats.MinPrice {
			stats.MinPrice = p
		}
		if p > stats.MaxPrice {
			stats.MaxPrice = p
		}
		if qtys[i] <= 0 {
			stats.ZeroQtys++
		}
	}

	for i := 1; i < len(times); i++ {
		t1 := times[i-1]
		t2 := times[i]

		deltaMs := t2 - t1
		if deltaMs < 0 {
			stats.Status = "ERR:TimeBack"
		}

		dt := time.Duration(deltaMs) * time.Millisecond
		if dt > stats.MaxGap {
			stats.MaxGap = dt
		}
		if dt > GapThresholdLarge {
			stats.GapCountLarge++
		} else if dt > GapThresholdSmall {
			stats.GapCountSmall++
		}
	}
}
```

// --- End File: check-data.go ---

// --- File: common.go ---

```go
package main

import (
	"encoding/binary"
	"io"
	"iter"
	"os"
	"path/filepath"
	"strconv"
	"sync"
	"unique"
	"unsafe"
)

// --- Infrastructure Constants ---
const (
	CPUThreads         = 24
	BaseDir            = "data"
	PxScale            = 100_000_000.0
	QtScale            = 100_000_000.0
	GNCChunkSize       = 65536
	GNCMagic           = "GNC3"
	GNCHeaderSize      = 32
	IdxMagic           = "QIDX"
	IdxVersion         = 1
	DefaultDayCapacity = 1_500_000
)

// SymbolHandle interns the symbol string.
var SymbolHandle = unique.Make(func() string {
	if s := os.Getenv("SYMBOL"); s != "" {
		return s
	}
	return "DOGEUSDT"
}())

func Symbol() string { return SymbolHandle.Value() }

// --- RAW DATA SCHEMA (SoA) ---
// Matches the binary file layout.
type DayColumns struct {
	Count               int
	Times               []int64
	Prices              []float64
	Qtys                []float64
	Sides               []int8
	Matches             []uint16
	ScratchQtyDict      []float64
	ScratchChunkOffsets []uint32
}

func (c *DayColumns) Reset() {
	c.Count = 0
	c.ScratchQtyDict = c.ScratchQtyDict[:0]
	c.ScratchChunkOffsets = c.ScratchChunkOffsets[:0]
}

// Pool for Raw Data Memory
var DayColumnPool = sync.Pool{
	New: func() any {
		return &DayColumns{
			Times:               make([]int64, 0, DefaultDayCapacity),
			Prices:              make([]float64, 0, DefaultDayCapacity),
			Qtys:                make([]float64, 0, DefaultDayCapacity),
			Sides:               make([]int8, 0, DefaultDayCapacity),
			Matches:             make([]uint16, 0, DefaultDayCapacity),
			ScratchQtyDict:      make([]float64, 0, 4096),
			ScratchChunkOffsets: make([]uint32, 0, 128),
		}
	},
}

// Helper: Cast slice to bytes for binary writing
func unsafeBytes[T any](s []T) []byte {
	if len(s) == 0 {
		return nil
	}
	elemSize := int(unsafe.Sizeof(*new(T)))
	return unsafe.Slice((*byte)(unsafe.Pointer(unsafe.SliceData(s))), len(s)*elemSize)
}

// inflateGNCToColumns loads the GNC3 blob into DayColumns.
func inflateGNCToColumns(rawBlob []byte, cols *DayColumns) (int, bool) {
	if len(rawBlob) < GNCHeaderSize {
		return 0, false
	}
	if string(rawBlob[0:4]) != GNCMagic {
		return 0, false
	}

	// 1. Metadata Parsing
	footerOffset := binary.LittleEndian.Uint64(rawBlob[24:32])
	if footerOffset >= uint64(len(rawBlob)) {
		return 0, false
	}

	dictBlob := rawBlob[footerOffset:]
	if len(dictBlob) < 4 {
		return 0, false
	}

	dictCount := binary.LittleEndian.Uint32(dictBlob[0:4])
	ptr := 4

	if uint64(ptr)+uint64(dictCount)*8+4 > uint64(len(dictBlob)) {
		return 0, false
	}

	if cap(cols.ScratchQtyDict) < int(dictCount) {
		cols.ScratchQtyDict = make([]float64, 0, int(dictCount))
	}
	qtyDict := cols.ScratchQtyDict[:dictCount]

	for i := 0; i < int(dictCount); i++ {
		qRaw := binary.LittleEndian.Uint64(dictBlob[ptr : ptr+8])
		qtyDict[i] = float64(qRaw) / QtScale
		ptr += 8
	}

	if len(dictBlob) < ptr+4 {
		return 0, false
	}
	chunkCount := binary.LittleEndian.Uint32(dictBlob[ptr : ptr+4])
	ptr += 4

	if uint64(ptr)+uint64(chunkCount)*4 > uint64(len(dictBlob)) {
		return 0, false
	}

	if cap(cols.ScratchChunkOffsets) < int(chunkCount) {
		cols.ScratchChunkOffsets = make([]uint32, 0, int(chunkCount))
	}
	chunkOffsets := cols.ScratchChunkOffsets[:chunkCount]

	for i := 0; i < int(chunkCount); i++ {
		chunkOffsets[i] = binary.LittleEndian.Uint32(dictBlob[ptr : ptr+4])
		ptr += 4
	}

	// 2. PASS 1: Calculate Total Rows
	totalRows := 0
	const ChunkHeaderSize = 18

	for _, off := range chunkOffsets {
		if uint64(off)+ChunkHeaderSize > uint64(len(rawBlob)) {
			return 0, false
		}
		n := int(binary.LittleEndian.Uint16(rawBlob[off : off+2]))
		totalRows += n
	}

	if totalRows == 0 {
		cols.Count = 0
		return 0, true
	}

	// 3. One-Time Allocation
	if cap(cols.Times) < totalRows {
		cols.Times = make([]int64, totalRows)
		cols.Prices = make([]float64, totalRows)
		cols.Qtys = make([]float64, totalRows)
		cols.Sides = make([]int8, totalRows)
		cols.Matches = make([]uint16, totalRows)
	}
	cols.Times = cols.Times[:totalRows]
	cols.Prices = cols.Prices[:totalRows]
	cols.Qtys = cols.Qtys[:totalRows]
	cols.Sides = cols.Sides[:totalRows]
	cols.Matches = cols.Matches[:totalRows]

	// 4. PASS 2: Indexed Writes
	writePtr := 0
	for _, off := range chunkOffsets {
		chunk := rawBlob[off:]
		n := int(binary.LittleEndian.Uint16(chunk[0:2]))
		baseT := int64(binary.LittleEndian.Uint64(chunk[2:10]))
		baseP := int64(binary.LittleEndian.Uint64(chunk[10:18]))

		pTime := ChunkHeaderSize
		pPrice := pTime + n*4
		pQty := pPrice + n*8
		pMatches := pQty + n*4
		pSide := pMatches + n*2

		if pSide > len(chunk) {
			return 0, false
		}

		tDeltas := unsafe.Slice((*int32)(unsafe.Pointer(&chunk[pTime])), n)
		pDeltas := unsafe.Slice((*int64)(unsafe.Pointer(&chunk[pPrice])), n)
		qIDs := unsafe.Slice((*uint32)(unsafe.Pointer(&chunk[pQty])), n)
		ms := unsafe.Slice((*uint16)(unsafe.Pointer(&chunk[pMatches])), n)
		sideBits := chunk[pSide:]

		dstT := cols.Times[writePtr : writePtr+n]
		dstP := cols.Prices[writePtr : writePtr+n]
		dstQ := cols.Qtys[writePtr : writePtr+n]
		dstS := cols.Sides[writePtr : writePtr+n]
		dstM := cols.Matches[writePtr : writePtr+n]

		lastT := baseT
		lastP := baseP

		for i := 0; i < n; i++ {
			lastT += int64(tDeltas[i])
			lastP += pDeltas[i]

			finalPrice := float64(lastP) / PxScale
			if finalPrice <= 1e-9 {
				finalPrice = 1e-9
			}

			dstT[i] = lastT
			dstP[i] = finalPrice

			qID := int(qIDs[i])
			if qID < len(qtyDict) {
				dstQ[i] = qtyDict[qID]
			} else {
				dstQ[i] = 0
			}

			dstM[i] = ms[i]

			b := sideBits[i/8]
			bit := int8((b >> (i % 8)) & 1)
			dstS[i] = bit*2 - 1
		}
		writePtr += n
	}

	cols.Count = totalRows
	return totalRows, true
}

type ofiTask struct {
	Year  int
	Month int
	Day   int
}

func discoverSymbols() iter.Seq[string] {
	return func(yield func(string) bool) {
		entries, err := os.ReadDir(BaseDir)
		if err != nil {
			return
		}
		for _, e := range entries {
			if !e.IsDir() {
				continue
			}
			name := e.Name()
			if name == "" || name[0] == '.' || name == "features" {
				continue
			}
			if !yield(name) {
				return
			}
		}
	}
}

func discoverTasks(sym string) iter.Seq[ofiTask] {
	return func(yield func(ofiTask) bool) {
		root := filepath.Join(BaseDir, sym)
		years, err := os.ReadDir(root)
		if err != nil {
			return
		}

		for _, yEnt := range years {
			if !yEnt.IsDir() {
				continue
			}
			yName := yEnt.Name()
			if len(yName) != 4 {
				continue
			}
			year, err := strconv.Atoi(yName)
			if err != nil {
				continue
			}
			yearDir := filepath.Join(root, yName)

			months, err := os.ReadDir(yearDir)
			if err != nil {
				continue
			}
			for _, mEnt := range months {
				if !mEnt.IsDir() {
					continue
				}
				mName := mEnt.Name()
				if len(mName) != 2 {
					continue
				}
				month, err := strconv.Atoi(mName)
				if err != nil {
					continue
				}

				idxPath := filepath.Join(yearDir, mName, "index.quantdev")
				f, err := os.Open(idxPath)
				if err != nil {
					continue
				}

				var hdr [16]byte
				if _, err := io.ReadFull(f, hdr[:]); err != nil {
					f.Close()
					continue
				}
				if string(hdr[0:4]) != IdxMagic {
					f.Close()
					continue
				}
				count := binary.LittleEndian.Uint64(hdr[8:])

				var row [26]byte
				for i := uint64(0); i < count; i++ {
					if _, err := io.ReadFull(f, row[:]); err != nil {
						break
					}
					day := int(binary.LittleEndian.Uint16(row[0:]))
					task := ofiTask{Year: year, Month: month, Day: day}
					if !yield(task) {
						f.Close()
						return
					}
				}
				f.Close()
			}
		}
	}
}
```

// --- End File: common.go ---

// --- File: data.go ---

```go
package main

import (
	"archive/zip"
	"bufio"
	"bytes"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"net/http"
	"os"
	"os/signal"
	"path/filepath"
	"strings"
	"sync"
	"sync/atomic"
	"time"
	"weak"
)

const (
	HostData   = "data.binance.vision"
	S3Prefix   = "data/futures/um"
	DataSet    = "aggTrades"
	FallbackDt = "2020-01-01"
)

var (
	httpClient *http.Client
	stopEvent  atomic.Bool

	// OPTIMIZED: Weak Map for Named Locks
	namedLocks = struct {
		sync.Mutex
		m map[string]weak.Pointer[sync.Mutex]
	}{m: make(map[string]weak.Pointer[sync.Mutex])}
)

var errNotFound = fmt.Errorf("404")

// GetDirLock uses weak pointers to prevent memory leaks on infinite symbols.
func GetDirLock(path string) *sync.Mutex {
	namedLocks.Lock()
	defer namedLocks.Unlock()

	if wp, ok := namedLocks.m[path]; ok {
		if ptr := wp.Value(); ptr != nil {
			return ptr
		}
		delete(namedLocks.m, path)
	}

	mu := &sync.Mutex{}
	namedLocks.m[path] = weak.Make(mu)
	return mu
}

// --- Ingestion Memory Pool ---

type IngestBuffers struct {
	Ts       []int64
	Ps       []int64
	Qs       []uint64
	Ms       []uint16
	Buys     []bool
	TDeltas  []int32
	PDeltas  []int64
	QIDs     []uint32
	SideBits []byte
}

var ingestBufferPool = sync.Pool{
	New: func() any {
		const cap = 1_000_000
		return &IngestBuffers{
			Ts:       make([]int64, 0, cap),
			Ps:       make([]int64, 0, cap),
			Qs:       make([]uint64, 0, cap),
			Ms:       make([]uint16, 0, cap),
			Buys:     make([]bool, 0, cap),
			TDeltas:  make([]int32, GNCChunkSize),
			PDeltas:  make([]int64, GNCChunkSize),
			QIDs:     make([]uint32, GNCChunkSize),
			SideBits: make([]byte, (GNCChunkSize+7)/8),
		}
	},
}

func (b *IngestBuffers) Reset() {
	b.Ts = b.Ts[:0]
	b.Ps = b.Ps[:0]
	b.Qs = b.Qs[:0]
	b.Ms = b.Ms[:0]
	b.Buys = b.Buys[:0]
}

func init() {
	tr := &http.Transport{
		MaxIdleConns:        100,
		MaxIdleConnsPerHost: 100,
		IdleConnTimeout:     90 * time.Second,
	}
	httpClient = &http.Client{
		Transport: tr,
		Timeout:   30 * time.Second,
	}
}

func runData() {
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, os.Interrupt)
	go func() {
		<-sigChan
		stopEvent.Store(true)
		fmt.Println("\n[warn] Stopping gracefully...")
	}()

	fmt.Printf("--- GNC-v3 Ingestion (Streaming) | Symbol: %s ---\n", Symbol())

	start, err := time.Parse("2006-01-02", FallbackDt)
	if err != nil {
		fmt.Printf("[fatal] invalid FallbackDt: %v\n", err)
		return
	}

	end := time.Now().UTC().AddDate(0, 0, -1)
	var days []time.Time
	for d := start; !d.After(end); d = d.AddDate(0, 0, 1) {
		days = append(days, d)
	}

	fmt.Printf("[job] Processing %d days using %d threads.\n", len(days), CPUThreads)

	jobs := make(chan time.Time, len(days))
	results := make(chan string, len(days))
	var wg sync.WaitGroup

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for d := range jobs {
				if stopEvent.Load() {
					return
				}
				results <- processDay(d)
			}
		}()
	}

	for _, d := range days {
		jobs <- d
	}
	close(jobs)
	wg.Wait()
	close(results)

	stats := make(map[string]int)
	for r := range results {
		parts := strings.SplitN(r, " ", 2)
		key := parts[0]
		stats[key]++
		if strings.HasPrefix(key, "error") {
			fmt.Println(r)
		}
	}
	fmt.Printf("\n[done] %v\n", stats)
}

func processDay(d time.Time) string {
	y, m, day := d.Year(), int(d.Month()), d.Day()
	dateStr := d.Format("2006-01-02")

	dirPath := filepath.Join(BaseDir, Symbol(), fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dirPath, "index.quantdev")
	dataPath := filepath.Join(dirPath, "data.quantdev")

	mu := GetDirLock(dirPath)
	mu.Lock()
	indexed := isIndexed(idxPath, day)
	mu.Unlock()
	if indexed {
		return "skip"
	}

	sym := Symbol()
	url := fmt.Sprintf("https://%s/%s/daily/%s/%s/%s-%s-%04d-%02d-%02d.zip",
		HostData, S3Prefix, DataSet, sym, sym, DataSet, y, m, day)

	zipBytes, err := download(url)
	if err != nil {
		if err == errNotFound {
			return "missing " + dateStr
		}
		return fmt.Sprintf("error_dl %s %v", dateStr, err)
	}

	bufs := ingestBufferPool.Get().(*IngestBuffers)
	bufs.Reset()
	defer ingestBufferPool.Put(bufs)

	gncBlob, count, err := streamZipToGNCBlob(zipBytes, bufs)
	if err != nil {
		return fmt.Sprintf("error_parse %s %v", dateStr, err)
	}
	if count == 0 {
		return "empty " + dateStr
	}

	sum := sha256.Sum256(gncBlob)
	cSum := binary.LittleEndian.Uint64(sum[:8])

	mu.Lock()
	defer mu.Unlock()

	if isIndexed(idxPath, day) {
		return "skip_race"
	}

	if err := os.MkdirAll(dirPath, 0755); err != nil {
		return "error_mkdir"
	}

	fData, err := os.OpenFile(dataPath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644)
	if err != nil {
		return "error_io"
	}

	stat, err := fData.Stat()
	if err != nil {
		fData.Close()
		return "error_stat"
	}
	offset := stat.Size()

	if _, err := fData.Write(gncBlob); err != nil {
		fData.Close()
		return "error_write"
	}
	fData.Close()

	if err := updateIndex(idxPath, day, offset, len(gncBlob), cSum); err != nil {
		return "error_idx"
	}

	return "ok " + dateStr
}

func streamZipToGNCBlob(zipData []byte, bufs *IngestBuffers) ([]byte, uint64, error) {
	r, err := zip.NewReader(bytes.NewReader(zipData), int64(len(zipData)))
	if err != nil {
		return nil, 0, err
	}

	for _, f := range r.File {
		if !strings.HasSuffix(f.Name, ".csv") {
			continue
		}
		rc, err := f.Open()
		if err != nil {
			continue
		}
		count, err := scanCSVToBuffers(rc, bufs)
		rc.Close()
		if err != nil {
			return nil, 0, err
		}
		return encodeGNC(bufs, count)
	}
	return nil, 0, fmt.Errorf("no csv found")
}

// scanCSVToBuffers using AVX-512 optimized bytes.IndexByte
func scanCSVToBuffers(r io.Reader, bufs *IngestBuffers) (int, error) {
	scanner := bufio.NewScanner(r)
	// Larger buffer to minimize IO calls
	buf := make([]byte, 0, 1024*1024)
	scanner.Buffer(buf, 10*1024*1024)

	count := 0
	firstLine := true

	for scanner.Scan() {
		line := scanner.Bytes()
		if len(line) == 0 {
			continue
		}
		if firstLine {
			firstLine = false
			if line[0] < '0' || line[0] > '9' {
				continue
			}
		}

		// Vectorized Parsing Logic
		var price, ts int64
		var qty uint64
		var firstID, lastID int64
		var isBuyerMaker bool

		start := 0
		col := 0

		// Unrolled column parser
		for col < 7 {
			idx := bytes.IndexByte(line[start:], ',')
			var field []byte
			if idx == -1 {
				field = line[start:]
			} else {
				field = line[start : start+idx]
			}

			switch col {
			case 1:
				price = fastParseFixed(field)
			case 2:
				rawQ := fastParseFixed(field)
				if rawQ < 0 {
					rawQ = -rawQ
				}
				qty = uint64(rawQ)
			case 3:
				firstID = fastParseInt(field)
			case 4:
				lastID = fastParseInt(field)
			case 5:
				ts = fastParseInt(field)
			case 6:
				if len(field) > 0 {
					c := field[0]
					if c == 't' || c == 'T' {
						isBuyerMaker = true
					}
				}
			}

			if idx == -1 {
				break
			}
			start += idx + 1
			col++
		}

		bufs.Ps = append(bufs.Ps, price)
		bufs.Qs = append(bufs.Qs, qty)
		bufs.Ts = append(bufs.Ts, ts)
		bufs.Buys = append(bufs.Buys, !isBuyerMaker)

		matches := int64(1)
		if lastID >= firstID {
			matches = lastID - firstID + 1
		}
		if matches > 65535 {
			matches = 65535
		}
		bufs.Ms = append(bufs.Ms, uint16(matches))

		count++
	}

	return count, scanner.Err()
}

func encodeGNC(bufs *IngestBuffers, count int) ([]byte, uint64, error) {
	if count == 0 {
		return nil, 0, nil
	}

	var buf bytes.Buffer
	buf.Grow(count * 20)

	baseTime := bufs.Ts[0]
	basePrice := bufs.Ps[0]

	buf.WriteString(GNCMagic)

	var scratch [8]byte
	binary.LittleEndian.PutUint32(scratch[:4], uint32(count))
	buf.Write(scratch[:4])

	binary.LittleEndian.PutUint64(scratch[:], uint64(baseTime))
	buf.Write(scratch[:])

	binary.LittleEndian.PutUint64(scratch[:], uint64(basePrice))
	buf.Write(scratch[:])

	footerOffsetPos := buf.Len()
	binary.LittleEndian.PutUint64(scratch[:], 0)
	buf.Write(scratch[:])

	qtyDict := make(map[uint64]uint32, 4096)
	var dictLog []uint64
	chunkOffsets := make([]uint32, 0)

	for i := 0; i < count; i += GNCChunkSize {
		end := i + GNCChunkSize
		if end > count {
			end = count
		}
		chunkOffsets = append(chunkOffsets, uint32(buf.Len()))
		if err := encodeChunk(&buf, bufs, i, end, qtyDict, &dictLog); err != nil {
			return nil, 0, err
		}
	}

	footerStart := buf.Len()
	binary.LittleEndian.PutUint32(scratch[:4], uint32(len(dictLog)))
	buf.Write(scratch[:4])
	for _, q := range dictLog {
		binary.LittleEndian.PutUint64(scratch[:], q)
		buf.Write(scratch[:])
	}
	binary.LittleEndian.PutUint32(scratch[:4], uint32(len(chunkOffsets)))
	buf.Write(scratch[:4])
	for _, off := range chunkOffsets {
		binary.LittleEndian.PutUint32(scratch[:4], off)
		buf.Write(scratch[:4])
	}
	finalBytes := buf.Bytes()
	binary.LittleEndian.PutUint64(finalBytes[footerOffsetPos:], uint64(footerStart))
	return finalBytes, uint64(count), nil
}

func encodeChunk(w *bytes.Buffer, bufs *IngestBuffers, start, end int, dict map[uint64]uint32, log *[]uint64) error {
	count := end - start
	ts := bufs.Ts[start:end]
	ps := bufs.Ps[start:end]
	qs := bufs.Qs[start:end]
	ms := bufs.Ms[start:end]
	buys := bufs.Buys[start:end]

	tDeltas := bufs.TDeltas[:count]
	pDeltas := bufs.PDeltas[:count]
	qIDs := bufs.QIDs[:count]
	sideBits := bufs.SideBits[:(count+7)/8]

	for k := range sideBits {
		sideBits[k] = 0
	}

	chunkBaseT := ts[0]
	chunkBaseP := ps[0]
	var lastT, lastP int64 = chunkBaseT, chunkBaseP

	tDeltas[0] = 0
	pDeltas[0] = 0

	getID := func(q uint64) uint32 {
		if id, ok := dict[q]; ok {
			return id
		}
		id := uint32(len(*log))
		dict[q] = id
		*log = append(*log, q)
		return id
	}

	qIDs[0] = getID(qs[0])
	if buys[0] {
		sideBits[0] |= 1
	}

	for i := 1; i < count; i++ {
		dt := ts[i] - lastT
		dp := ps[i] - lastP
		if dt > 2147483647 || dt < -2147483648 {
			return fmt.Errorf("time delta overflow")
		}
		tDeltas[i] = int32(dt)
		lastT = ts[i]
		pDeltas[i] = dp
		lastP = ps[i]
		qIDs[i] = getID(qs[i])
		if buys[i] {
			sideBits[i/8] |= (1 << (i % 8))
		}
	}

	var head [18]byte
	binary.LittleEndian.PutUint16(head[0:], uint16(count))
	binary.LittleEndian.PutUint64(head[2:], uint64(chunkBaseT))
	binary.LittleEndian.PutUint64(head[10:], uint64(chunkBaseP))
	w.Write(head[:])

	w.Write(unsafeBytes(tDeltas))
	w.Write(unsafeBytes(pDeltas))
	w.Write(unsafeBytes(qIDs))
	w.Write(unsafeBytes(ms))
	w.Write(sideBits)

	return nil
}

// Fast fixed-point parser, avoids float conversion overhead
func fastParseFixed(b []byte) int64 {
	var num int64
	var seenDot bool
	var dec int
	for _, c := range b {
		if c == '.' {
			seenDot = true
			continue
		}
		if c < '0' || c > '9' {
			continue
		}
		num = num*10 + int64(c-'0')
		if seenDot {
			dec++
		}
	}
	for dec < 8 {
		num *= 10
		dec++
	}
	return num
}

func fastParseInt(b []byte) int64 {
	var n int64
	for _, c := range b {
		if c < '0' || c > '9' {
			continue
		}
		n = n*10 + int64(c-'0')
	}
	return n
}

func download(url string) ([]byte, error) {
	var lastErr error
	for attempt := 0; attempt < 3; attempt++ {
		resp, err := httpClient.Get(url)
		if err != nil {
			lastErr = err
			time.Sleep(100 * time.Millisecond)
			continue
		}
		if resp.StatusCode == 404 {
			resp.Body.Close()
			return nil, errNotFound
		}
		if resp.StatusCode != 200 {
			resp.Body.Close()
			lastErr = fmt.Errorf("status %d", resp.StatusCode)
			time.Sleep(100 * time.Millisecond)
			continue
		}
		data, err := io.ReadAll(resp.Body)
		resp.Body.Close()
		if err != nil {
			lastErr = err
			time.Sleep(100 * time.Millisecond)
			continue
		}
		return data, nil
	}
	return nil, lastErr
}

func isIndexed(idxPath string, day int) bool {
	f, err := os.Open(idxPath)
	if err != nil {
		return false
	}
	defer f.Close()
	return checkIndex(f, day)
}

func checkIndex(f *os.File, day int) bool {
	var hdr [16]byte
	if _, err := io.ReadFull(f, hdr[:]); err != nil {
		return false
	}
	if string(hdr[0:4]) != IdxMagic {
		return false
	}
	count := binary.LittleEndian.Uint64(hdr[8:])
	var row [26]byte
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row[:]); err != nil {
			return false
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return true
		}
	}
	return false
}

func updateIndex(idxPath string, day int, offset int64, length int, csum uint64) error {
	f, err := os.OpenFile(idxPath, os.O_CREATE|os.O_RDWR, 0644)
	if err != nil {
		return err
	}
	defer f.Close()

	stat, err := f.Stat()
	if err != nil {
		return err
	}
	if stat.Size() == 0 {
		var hdr [16]byte
		copy(hdr[0:], IdxMagic)
		binary.LittleEndian.PutUint32(hdr[4:], uint32(IdxVersion))
		if _, err := f.Write(hdr[:]); err != nil {
			return err
		}
	}

	if _, err := f.Seek(8, io.SeekStart); err != nil {
		return err
	}
	var count uint64
	if err := binary.Read(f, binary.LittleEndian, &count); err != nil {
		return err
	}
	if _, err := f.Seek(0, io.SeekEnd); err != nil {
		return err
	}
	var row [26]byte
	binary.LittleEndian.PutUint16(row[0:], uint16(day))
	binary.LittleEndian.PutUint64(row[2:], uint64(offset))
	binary.LittleEndian.PutUint64(row[10:], uint64(length))
	binary.LittleEndian.PutUint64(row[18:], csum)
	if _, err := f.Write(row[:]); err != nil {
		return err
	}
	if _, err := f.Seek(8, io.SeekStart); err != nil {
		return err
	}
	return binary.Write(f, binary.LittleEndian, count+1)
}

func loadRawGNC(sym string, t ofiTask, buf *[]byte) ([]byte, bool) {
	dir := filepath.Join(BaseDir, sym, fmt.Sprintf("%04d", t.Year), fmt.Sprintf("%02d", t.Month))
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")

	offset, length := findBlobOffset(idxPath, t.Day)
	if length == 0 {
		return nil, false
	}

	f, err := os.Open(dataPath)
	if err != nil {
		return nil, false
	}
	defer f.Close()

	st, err := f.Stat()
	if err != nil {
		return nil, false
	}
	if int64(offset+length) > st.Size() {
		return nil, false
	}

	need := int(length)
	if cap(*buf) < need {
		*buf = make([]byte, need)
	}
	b := (*buf)[:need]

	if _, err := f.Seek(int64(offset), io.SeekStart); err != nil {
		return nil, false
	}
	if _, err := io.ReadFull(f, b); err != nil {
		return nil, false
	}
	return b, true
}

func findBlobOffset(idxPath string, day int) (uint64, uint64) {
	f, err := os.Open(idxPath)
	if err != nil {
		return 0, 0
	}
	defer f.Close()

	var hdr [16]byte
	if _, err := io.ReadFull(f, hdr[:]); err != nil {
		return 0, 0
	}
	count := binary.LittleEndian.Uint64(hdr[8:])
	var row [26]byte
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row[:]); err != nil {
			break
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return binary.LittleEndian.Uint64(row[2:]), binary.LittleEndian.Uint64(row[10:])
		}
	}
	return 0, 0
}
```

// --- End File: data.go ---

// --- File: main.go ---

```go
package main

import (
	"fmt"
	"os"
	"runtime"
	"runtime/debug"
	"time"
)

func main() {
	runtime.GOMAXPROCS(CPUThreads)
	const ramLimit = 24 * 1024 * 1024 * 1024
	debug.SetMemoryLimit(ramLimit)

	if len(os.Args) < 2 {
		printHelp()
		os.Exit(1)
	}

	start := time.Now()

	fmt.Printf("%s | Env: %s/%s | Threads: %d | RAM Limit: 24GB | GOGC: %s | GOAMD64: %s | GOEXP: %s\n",
		runtime.Version(),
		runtime.GOOS, runtime.GOARCH,
		runtime.GOMAXPROCS(0),
		os.Getenv("GOGC"),
		os.Getenv("GOAMD64"),
		os.Getenv("GOEXPERIMENT"),
	)

	cmd := os.Args[1]

	switch cmd {
	case "test":
		runTest()
	case "check":
		runCheck()
	case "data":
		runData()
	case "bench":
		runBenchmark()
	default:
		fmt.Printf("Unknown command: %s\n", cmd)
		printHelp()
		os.Exit(1)
	}

	fmt.Printf("\n[sys] Execution Time: %s | Mem: %s\n", time.Since(start), getMemUsage())
}

func printHelp() {
	fmt.Println("Usage: go run . [command]")
	fmt.Println("  data   - Download and process raw aggTrades data (GNC-v3)")
	fmt.Println("  test   - Unified metrics + math study on GNC data")
	fmt.Println("  bench  - Synthetic end-to-end performance benchmark")
	fmt.Println("  check  - Scan GNC data for gaps / integrity issues")
}

func getMemUsage() string {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return fmt.Sprintf("%d MB", m.Alloc/1024/1024)
}
```

// --- End File: main.go ---

// --- File: math.go ---

```go
package main

import (
	"math"
	"sync"
)

// --- Constants & Hyperparameters ---
const (
	// Leaky Integrator Decay (Golden Ratio in seconds)
	TauPressure  = 0.618
	TauIntensity = 0.618
	TauVol       = 1.000

	// Kalman Filters
	KalmanR       = 0.25 // Measurement noise
	KalmanQStatic = 0.05 // Static process noise rate
	KalmanQBase   = 0.01 // Adaptive base noise
	KalmanAlpha   = 5.0  // Adaptive scaling factor for error^2

	// Regime Shift (CUSUM)
	CusumDriftK = 0.1 // Drift tolerance
	CusumResetH = 5.0 // Threshold to declare shift and reset
)

// --- 1. Math Workspace & Memory ---

// SortPair is used for quantile sorting (metrics) but allocated in workspace.
type SortPair struct {
	S, R float64
}

type MathWorkspace struct {
	// Pre-calculated vectors (Context)
	DT     []float64
	DP     []float64
	LogRet []float64
	RawTCI []float64

	// Scratch for sorting/stats (Shared usage)
	MeanBuf []float64
	SortBuf []SortPair
}

func (ws *MathWorkspace) Ensure(n int) {
	if cap(ws.DT) < n {
		ws.DT = make([]float64, n)
		ws.DP = make([]float64, n)
		ws.LogRet = make([]float64, n)
		ws.RawTCI = make([]float64, n)
		ws.MeanBuf = make([]float64, n)
		// Ensure SortBuf capacity
		ws.SortBuf = make([]SortPair, n)
	}
	ws.DT = ws.DT[:n]
	ws.DP = ws.DP[:n]
	ws.LogRet = ws.LogRet[:n]
	ws.RawTCI = ws.RawTCI[:n]
	ws.MeanBuf = ws.MeanBuf[:n]
	ws.SortBuf = ws.SortBuf[:n]
}

// Output Buffer Pool
type SignalBuffers struct {
	Data [][]float64
}

var SignalBufferPool = sync.Pool{
	New: func() any {
		return &SignalBuffers{
			Data: make([][]float64, 0),
		}
	},
}

// --- 2. The Bridge: Raw -> Math Context ---

type MathContext struct {
	Count int
	// Raw (from DayColumns)
	Prices  []float64
	Qtys    []float64
	Sides   []int8
	Matches []uint16
	// Derived (calculated in PrepareMathContext)
	DT     []float64
	DP     []float64
	LogRet []float64
	RawTCI []float64
}

// PrepareMathContext transforms the rigid DayColumns into the fluid MathContext.
func PrepareMathContext(cols *DayColumns, ws *MathWorkspace) *MathContext {
	n := cols.Count
	ws.Ensure(n)

	times := cols.Times
	prices := cols.Prices
	sides := cols.Sides

	// Optimization: Pointers for BCE (Bounds Check Elimination)
	pDT := ws.DT
	pDP := ws.DP
	pRet := ws.LogRet
	pTci := ws.RawTCI

	prevT := times[0]
	prevP := prices[0]
	const minDTsec = 1e-3

	for i := 0; i < n; i++ {
		// 1. Delta Time
		t := times[i]
		dSec := float64(t-prevT) * 0.001
		if dSec < minDTsec {
			dSec = minDTsec
		}
		pDT[i] = dSec

		// 2. Delta Price & Returns
		p := prices[i]
		pDP[i] = p - prevP

		if prevP > 0 && p > 0 {
			pRet[i] = math.Log(p / prevP)
		} else {
			pRet[i] = 0
		}

		// 3. Raw TCI (Cast int8 to float64)
		pTci[i] = float64(sides[i])

		prevT = t
		prevP = p
	}

	return &MathContext{
		Count:   n,
		Prices:  prices,
		Qtys:    cols.Qtys,
		Sides:   sides,
		Matches: cols.Matches,
		DT:      pDT,
		DP:      pDP,
		LogRet:  pRet,
		RawTCI:  pTci,
	}
}

// --- 3. The 7 Survivors (Generators) ---

type Generator interface {
	Name() string
	Reset()
	Update(ctx *MathContext, out []float64)
}

// 1. Raw TCI (Baseline)
type GenRawTCI struct{}

func (g *GenRawTCI) Name() string { return "Raw_TCI" }
func (g *GenRawTCI) Reset()       {}
func (g *GenRawTCI) Update(ctx *MathContext, out []float64) {
	copy(out, ctx.RawTCI)
}

// 2. Static Continuous-Time Kalman
type GenStaticKalman struct {
	x, p float64
}

func (g *GenStaticKalman) Name() string { return "Kalman_Static" }
func (g *GenStaticKalman) Reset()       { g.x = 0; g.p = 1.0 }
func (g *GenStaticKalman) Update(ctx *MathContext, out []float64) {
	x, p := g.x, g.p
	for i := 0; i < ctx.Count; i++ {
		p = p + KalmanQStatic*ctx.DT[i]
		k := p / (p + KalmanR)
		y := ctx.RawTCI[i]
		x = x + k*(y-x)
		p = (1.0 - k) * p
		out[i] = x
	}
	g.x, g.p = x, p
}

// 3. Adaptive Continuous-Time Kalman
type GenAdaptiveKalman struct {
	x, p float64
}

func (g *GenAdaptiveKalman) Name() string { return "Kalman_Adaptive" }
func (g *GenAdaptiveKalman) Reset()       { g.x = 0; g.p = 1.0 }
func (g *GenAdaptiveKalman) Update(ctx *MathContext, out []float64) {
	x, p := g.x, g.p
	for i := 0; i < ctx.Count; i++ {
		errRaw := ctx.RawTCI[i] - x
		qAdaptive := KalmanQBase + KalmanAlpha*(errRaw*errRaw)
		p = p + qAdaptive*ctx.DT[i]
		k := p / (p + KalmanR)
		x = x + k*errRaw
		p = (1.0 - k) * p
		out[i] = x
	}
	g.x, g.p = x, p
}

// 4. Leaky Integrator (Pressure)
type GenLeakyIntegrator struct {
	s float64
}

func (g *GenLeakyIntegrator) Name() string { return "Pressure_TCI" }
func (g *GenLeakyIntegrator) Reset()       { g.s = 0 }
func (g *GenLeakyIntegrator) Update(ctx *MathContext, out []float64) {
	s := g.s
	for i := 0; i < ctx.Count; i++ {
		decay := math.Exp(-ctx.DT[i] / TauPressure)
		s = ctx.RawTCI[i] + s*decay
		out[i] = s
	}
	g.s = s
}

// 5. Intensity Imbalance
type GenIntensityImbalance struct {
	lamBuy, lamSell float64
}

func (g *GenIntensityImbalance) Name() string { return "Intensity_Imb" }
func (g *GenIntensityImbalance) Reset()       { g.lamBuy = 0; g.lamSell = 0 }
func (g *GenIntensityImbalance) Update(ctx *MathContext, out []float64) {
	lb, ls := g.lamBuy, g.lamSell
	for i := 0; i < ctx.Count; i++ {
		decay := math.Exp(-ctx.DT[i] / TauIntensity)
		lb *= decay
		ls *= decay
		if ctx.RawTCI[i] > 0 {
			lb += 1.0
		} else {
			ls += 1.0
		}
		sum := lb + ls
		if sum < 1e-9 {
			out[i] = 0
		} else {
			out[i] = (lb - ls) / sum
		}
	}
	g.lamBuy, g.lamSell = lb, ls
}

// 6. Instantaneous Volatility
type GenInstantVol struct {
	v float64
}

func (g *GenInstantVol) Name() string { return "Instant_Vol" }
func (g *GenInstantVol) Reset()       { g.v = 0 }
func (g *GenInstantVol) Update(ctx *MathContext, out []float64) {
	v := g.v
	for i := 0; i < ctx.Count; i++ {
		decay := math.Exp(-ctx.DT[i] / TauVol)
		r := ctx.LogRet[i]
		v = (r * r) + v*decay
		out[i] = math.Sqrt(v)
	}
	g.v = v
}

// 7. Regime Shift CUSUM
type GenCUSUM struct {
	cPos, cNeg float64
}

func (g *GenCUSUM) Name() string { return "Regime_CUSUM" }
func (g *GenCUSUM) Reset()       { g.cPos = 0; g.cNeg = 0 }
func (g *GenCUSUM) Update(ctx *MathContext, out []float64) {
	cp, cn := g.cPos, g.cNeg
	for i := 0; i < ctx.Count; i++ {
		s := ctx.RawTCI[i]
		cp = math.Max(0, cp+s-CusumDriftK)
		cn = math.Min(0, cn+s+CusumDriftK)
		res := 0.0
		if cp > CusumResetH {
			res = 1.0
			cp = 0
			cn = 0
		} else if cn < -CusumResetH {
			res = -1.0
			cp = 0
			cn = 0
		} else {
			if cp > -cn {
				res = cp / CusumResetH
			} else {
				res = cn / CusumResetH
			}
		}
		out[i] = res
	}
	g.cPos, g.cNeg = cp, cn
}

// --- Registry ---
func GetCorePrimitives() []Generator {
	return []Generator{
		&GenRawTCI{},
		&GenStaticKalman{},
		&GenAdaptiveKalman{},
		&GenLeakyIntegrator{},
		&GenIntensityImbalance{},
		&GenInstantVol{},
		&GenCUSUM{},
	}
}

// --- Statistical Structures (Kept for compatibility with test.go) ---
type MathDist struct {
	Count, Min, Max, Sum, SumSq, Last float64
	Outliers                          int64
}

func InitMathDists(n int) []MathDist {
	d := make([]MathDist, n)
	for i := range d {
		d[i].Min = math.MaxFloat64
		d[i].Max = -math.MaxFloat64
	}
	return d
}
```

// --- End File: math.go ---

// --- File: metrics.go ---

```go
package main

import (
	"math"
	"slices"
)

// MetricStats focuses on Signal Physics and Information Content.
type MetricStats struct {
	Count int

	// 1. Predictive Power (Information Coefficient)
	IC_NextTick float64 // Correlation with next 1 tick return (Microstructure)
	IC_10Tick   float64 // Correlation with next 10 ticks (Short Trend)
	IC_100Tick  float64 // Correlation with next 100 ticks (Alpha)

	// 2. Signal Dynamics (Behavior)
	Autocorrelation float64 // Lag-1 AutoCorr: >0.9=Stable, <0.0=MeanReverting
	FlipRate        float64 // % of updates where sign changes (Jitter)
	ActivityRate    float64 // % of time signal is non-zero (Sparsity)

	// 3. Information Quality
	SignalNoiseRatio float64 // Mean / StdDev (Consistency of bias)
	OutlierRate      float64 // % of values > 3 sigma

	// 4. Lead/Lag Structure
	Monotonicity float64 // Rank correlation of Signal vs Return Buckets
}

type Moments struct {
	Count float64

	// Sums for Correlation (Signal side)
	SumSig   float64
	SumSqSig float64

	// Returns at different horizons
	SumRet1   float64
	SumSqRet1 float64
	SumProd1  float64 // Sig * Ret1

	SumRet10   float64
	SumSqRet10 float64
	SumProd10  float64 // Sig * Ret10

	SumRet100   float64
	SumSqRet100 float64
	SumProd100  float64 // Sig * Ret100

	// Stability Moments
	SumProdLag1 float64 // Sig[t] * Sig[t-1] (Autocorr)
	Flips       float64 // Count of sign changes
	NonZero     float64 // Count of non-zero signals
	Outliers    float64 // Count of extreme values
}

func (m *Moments) Add(m2 Moments) {
	m.Count += m2.Count
	m.SumSig += m2.SumSig
	m.SumSqSig += m2.SumSqSig

	m.SumRet1 += m2.SumRet1
	m.SumSqRet1 += m2.SumSqRet1
	m.SumProd1 += m2.SumProd1

	m.SumRet10 += m2.SumRet10
	m.SumSqRet10 += m2.SumSqRet10
	m.SumProd10 += m2.SumProd10

	m.SumRet100 += m2.SumRet100
	m.SumSqRet100 += m2.SumSqRet100
	m.SumProd100 += m2.SumProd100

	m.SumProdLag1 += m2.SumProdLag1
	m.Flips += m2.Flips
	m.NonZero += m2.NonZero
	m.Outliers += m2.Outliers
}

// CalcMomentsVectors computes physics stats in a single pass.
func CalcMomentsVectors(sigs, rets1, rets10, rets100 []float64) Moments {
	var m Moments
	n := len(sigs)
	if n == 0 {
		return m
	}

	// BCE Hint
	_ = rets1[n-1]

	var prevSig float64

	for i := 0; i < n; i++ {
		s := sigs[i]

		m.SumSig += s
		m.SumSqSig += s * s

		if s != 0 {
			m.NonZero++
		}
		if math.Abs(s) > 3.0 {
			m.Outliers++
		}

		// 1-Tick IC
		r1 := rets1[i]
		m.SumRet1 += r1
		m.SumSqRet1 += r1 * r1
		m.SumProd1 += s * r1

		// 10-Tick IC
		if i < len(rets10) {
			r10 := rets10[i]
			m.SumRet10 += r10
			m.SumSqRet10 += r10 * r10
			m.SumProd10 += s * r10
		}

		// 100-Tick IC
		if i < len(rets100) {
			r100 := rets100[i]
			m.SumRet100 += r100
			m.SumSqRet100 += r100 * r100
			m.SumProd100 += s * r100
		}

		// Dynamics
		if i > 0 {
			m.SumProdLag1 += s * prevSig
			// Flip detection: Crossing zero
			if (s > 0 && prevSig < 0) || (s < 0 && prevSig > 0) {
				m.Flips++
			}
		}
		prevSig = s
	}
	m.Count = float64(n)
	return m
}

func FinalizeMetrics(m Moments) MetricStats {
	if m.Count <= 1 {
		return MetricStats{}
	}

	stats := MetricStats{Count: int(m.Count)}
	N := m.Count

	// Helper for Pearson Correlation
	corr := func(sumX, sumSqX, sumY, sumSqY, sumXY float64) float64 {
		num := N*sumXY - sumX*sumY
		denX := N*sumSqX - sumX*sumX
		denY := N*sumSqY - sumY*sumY
		if denX <= 1e-18 || denY <= 1e-18 {
			return 0
		}
		return num / math.Sqrt(denX*denY)
	}

	// 1. Prediction (IC)
	stats.IC_NextTick = corr(m.SumSig, m.SumSqSig, m.SumRet1, m.SumSqRet1, m.SumProd1)
	stats.IC_10Tick = corr(m.SumSig, m.SumSqSig, m.SumRet10, m.SumSqRet10, m.SumProd10)
	stats.IC_100Tick = corr(m.SumSig, m.SumSqSig, m.SumRet100, m.SumSqRet100, m.SumProd100)

	// 2. Dynamics
	// Autocorrelation (Lag-1)
	// We approximate using SumSig/SumSqSig for Y as well, as Lag-1 shift is negligible for large N
	stats.Autocorrelation = corr(m.SumSig, m.SumSqSig, m.SumSig, m.SumSqSig, m.SumProdLag1)

	stats.FlipRate = m.Flips / N
	stats.ActivityRate = m.NonZero / N

	// 3. Quality
	mean := m.SumSig / N
	variance := (m.SumSqSig / N) - mean*mean
	if variance > 0 {
		stats.SignalNoiseRatio = math.Abs(mean) / math.Sqrt(variance)
	}
	stats.OutlierRate = m.Outliers / N

	return stats
}

// --- Bucket Logic (Monotonicity) ---

type BucketResult struct {
	ID        int
	AvgSig    float64
	AvgRetBps float64
	Count     int
}

type BucketAgg struct {
	Count     int
	SumSig    float64
	SumRetBps float64
}

func (ba *BucketAgg) Add(br BucketResult) {
	if br.Count <= 0 {
		return
	}
	ba.Count += br.Count
	ba.SumSig += br.AvgSig * float64(br.Count)
	ba.SumRetBps += br.AvgRetBps * float64(br.Count)
}

func (ba BucketAgg) Finalize(id int) BucketResult {
	if ba.Count == 0 {
		return BucketResult{ID: id}
	}
	den := float64(ba.Count)
	return BucketResult{
		ID:        id,
		AvgSig:    ba.SumSig / den,
		AvgRetBps: ba.SumRetBps / den,
		Count:     ba.Count,
	}
}

func ComputeQuantilesStrided(sigs, rets []float64, numBuckets, stride int, scratch *MathWorkspace) []BucketResult {
	n := len(sigs)
	if n == 0 || numBuckets <= 0 {
		return nil
	}

	if n < 10000 {
		stride = 1
	} else if stride < 1 {
		stride = 1
	}

	estSize := n / stride
	if cap(scratch.SortBuf) < estSize {
		scratch.SortBuf = make([]SortPair, estSize)
	}
	pairs := scratch.SortBuf[:0]

	for i := 0; i < n; i += stride {
		pairs = append(pairs, SortPair{S: sigs[i], R: rets[i]})
	}
	if len(pairs) == 0 {
		return nil
	}

	slices.SortFunc(pairs, func(a, b SortPair) int {
		if a.S < b.S {
			return -1
		}
		if a.S > b.S {
			return 1
		}
		return 0
	})

	subN := len(pairs)
	results := make([]BucketResult, numBuckets)
	bucketSize := subN / numBuckets
	if bucketSize == 0 {
		bucketSize = 1
	}

	for b := 0; b < numBuckets; b++ {
		start := b * bucketSize
		end := start + bucketSize
		if b == numBuckets-1 || end > subN {
			end = subN
		}

		var sumS, sumR float64
		count := 0
		for i := start; i < end; i++ {
			sumS += pairs[i].S
			sumR += pairs[i].R
			count++
		}
		if count > 0 {
			results[b] = BucketResult{
				ID:        b + 1,
				AvgSig:    sumS / float64(count),
				AvgRetBps: (sumR / float64(count)) * 10000.0,
				Count:     count * stride,
			}
		}
	}
	return results
}

func ComputeBucketMonotonicity(bucketAggs []BucketAgg) float64 {
	var brs []BucketResult
	for i, agg := range bucketAggs {
		br := agg.Finalize(i + 1)
		if br.Count > 0 {
			brs = append(brs, br)
		}
	}
	n := len(brs)
	if n < 2 {
		return math.NaN()
	}

	type kv struct {
		idx int
		val float64
	}
	ranks := make([]kv, n)
	for i, br := range brs {
		ranks[i] = kv{idx: i, val: br.AvgRetBps}
	}

	slices.SortFunc(ranks, func(a, b kv) int {
		if a.val < b.val {
			return -1
		}
		if a.val > b.val {
			return 1
		}
		return 0
	})

	yRank := make([]float64, n)
	for rank, kv := range ranks {
		yRank[kv.idx] = float64(rank + 1)
	}

	var sumD2 float64
	for i := 0; i < n; i++ {
		xRank := float64(i + 1)
		d := xRank - yRank[i]
		sumD2 += d * d
	}
	nf := float64(n)
	return 1.0 - (6.0*sumD2)/(nf*(nf*nf-1.0))
}
```

// --- End File: metrics.go ---

// --- File: test.go ---

```go
package main

import (
	"fmt"
	"math"
	"os"
	"sort"
	"sync"
	"text/tabwriter"
	"time"
)

func writeMetricsLog(f *os.File, msg string) {
	fmt.Print(msg)
	_, _ = f.WriteString(msg)
}

// SignalSummary now captures Physics properties
type SignalSummary struct {
	Symbol       string
	SignalName   string
	IC_1         float64 // 1-tick Prediction
	IC_100       float64 // 100-tick Prediction
	AutoCorr     float64 // Stability
	FlipRate     float64 // Jitter
	Monotonicity float64 // Reliability at tails
}

type DailyStats struct {
	Moments       Moments
	BucketResults []BucketResult
}

type TaskResult struct {
	DayIndex int
	Stats    []DailyStats
}

func runTest() {
	start := time.Now()
	gens := GetCorePrimitives()

	fmt.Println(">>> UNIFIED STUDY: SIGNAL PHYSICS & DYNAMICS <<<")
	fmt.Printf("[config] Threads=%d | Signals=%d\n", CPUThreads, len(gens))
	fmt.Println("[info] Metrics: IC(Horizon), AutoCorr(Stability), Flip%(Jitter), SNR")

	var symbols []string
	for sym := range discoverSymbols() {
		symbols = append(symbols, sym)
	}
	if len(symbols) == 0 {
		fmt.Printf("[fatal] No symbols found in %s\n", BaseDir)
		return
	}

	reportFile, err := os.Create("Unified_Report_Physics.txt")
	if err != nil {
		fmt.Printf("[fatal] cannot create report: %v\n", err)
		return
	}
	defer reportFile.Close()

	totalDays := 0
	var allSummaries []SignalSummary

	for _, sym := range symbols {
		writeMetricsLog(reportFile, fmt.Sprintf("\n>>> SYMBOL: %s <<<\n", sym))
		days, summaries := runUnifiedParallel(sym, reportFile)
		totalDays += days
		allSummaries = append(allSummaries, summaries...)
	}

	writeMetricsLog(reportFile, fmt.Sprintf("\n[done] Completed in %s | Total Days: %d\n", time.Since(start), totalDays))
	printPhysicsWinners(allSummaries)
}

func runUnifiedParallel(sym string, report *os.File) (int, []SignalSummary) {
	const numBuckets = 10
	const stride = 1000

	generators := GetCorePrimitives()
	numSignals := len(generators)
	metricNames := make([]string, numSignals)
	for i, g := range generators {
		metricNames[i] = g.Name()
	}

	var tasks []ofiTask
	for t := range discoverTasks(sym) {
		tasks = append(tasks, t)
	}
	numDays := len(tasks)
	if numDays == 0 {
		return 0, nil
	}

	results := make([]*TaskResult, numDays)
	taskChan := make(chan int, numDays)
	var wg sync.WaitGroup

	for i := 0; i < numDays; i++ {
		taskChan <- i
	}
	close(taskChan)

	// Worker Pool
	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()

			// Pools
			cols := DayColumnPool.Get().(*DayColumns)
			ws := &MathWorkspace{}
			sb := SignalBufferPool.Get().(*SignalBuffers)

			// Thread-Local Reuse
			var gncBuf []byte
			var retBuf []float64
			localGens := GetCorePrimitives()

			defer func() {
				DayColumnPool.Put(cols)
				SignalBufferPool.Put(sb)
			}()

			for taskIdx := range taskChan {
				t := tasks[taskIdx]
				res := processDayTask(sym, t, taskIdx, numSignals, numBuckets, stride,
					cols, sb, ws, &gncBuf, &retBuf, localGens)
				if res != nil {
					results[taskIdx] = res
				}
			}
		}()
	}

	wg.Wait()

	// Aggregation
	globalMoments := make([]Moments, numSignals)
	bucketAggs := make([][]BucketAgg, numSignals)
	for i := 0; i < numSignals; i++ {
		bucketAggs[i] = make([]BucketAgg, numBuckets)
	}

	validDays := 0
	for _, res := range results {
		if res == nil {
			continue
		}
		validDays++
		for sIdx, stats := range res.Stats {
			globalMoments[sIdx].Add(stats.Moments)
			for b, br := range stats.BucketResults {
				if b < len(bucketAggs[sIdx]) {
					bucketAggs[sIdx][b].Add(br)
				}
			}
		}
	}

	summaries := make([]SignalSummary, 0, numSignals)

	// PHYSICS REPORT HEADER
	w := tabwriter.NewWriter(report, 0, 0, 1, ' ', 0)
	fmt.Fprintln(w, "SIGNAL\tIC(1)\tIC(10)\tIC(100)\tAUTO\tFLIP%\tMONO\tSNR")
	fmt.Fprintln(w, "------\t-----\t------\t-------\t----\t-----\t----\t---")

	for sIdx, sigName := range metricNames {
		stats := FinalizeMetrics(globalMoments[sIdx])
		mono := ComputeBucketMonotonicity(bucketAggs[sIdx])

		fmt.Fprintf(w, "%s\t%.4f\t%.4f\t%.4f\t%.3f\t%.1f%%\t%.2f\t%.2f\n",
			shortName(sigName),
			stats.IC_NextTick, stats.IC_10Tick, stats.IC_100Tick,
			stats.Autocorrelation, stats.FlipRate*100, mono, stats.SignalNoiseRatio)

		summaries = append(summaries, SignalSummary{
			Symbol:       sym,
			SignalName:   sigName,
			IC_1:         stats.IC_NextTick,
			IC_100:       stats.IC_100Tick,
			AutoCorr:     stats.Autocorrelation,
			FlipRate:     stats.FlipRate,
			Monotonicity: mono,
		})
	}
	w.Flush()
	fmt.Fprintln(report, "")

	return validDays, summaries
}

func shortName(s string) string {
	if len(s) > 20 {
		return s[:17] + "..."
	}
	return s
}

func processDayTask(sym string, t ofiTask, taskIdx, numSignals, numBuckets, stride int,
	cols *DayColumns, sb *SignalBuffers, ws *MathWorkspace,
	gncBuf *[]byte, retBuf *[]float64,
	gens []Generator) *TaskResult {

	gncBlob, ok := loadRawGNC(sym, t, gncBuf)
	if !ok || len(gncBlob) == 0 {
		return nil
	}

	cols.Reset()
	rowCount, ok := inflateGNCToColumns(gncBlob, cols)
	if !ok || rowCount < 2 {
		return nil
	}
	n := rowCount
	nRet := n - 1

	mathCtx := PrepareMathContext(cols, ws)

	// --- Return Horizons Calculation ---
	// Reuse retBuf for Ret1
	if cap(*retBuf) < nRet {
		*retBuf = make([]float64, nRet)
	}
	rets1 := (*retBuf)[:nRet]
	copy(rets1, mathCtx.LogRet[:nRet])

	// Create Horizon Views
	// We allocate locally to keep math logic simple and thread-safe
	rets10 := make([]float64, nRet)
	rets100 := make([]float64, nRet)

	// Horizon Calc Loop
	for i := 0; i < nRet; i++ {
		// 10-tick lookahead sum
		sum10 := 0.0
		lim10 := i + 10
		if lim10 > nRet {
			lim10 = nRet
		}
		for k := i; k < lim10; k++ {
			sum10 += rets1[k]
		}
		rets10[i] = sum10

		// 100-tick lookahead sum
		sum100 := 0.0
		lim100 := i + 100
		if lim100 > nRet {
			lim100 = nRet
		}
		for k := i; k < lim100; k++ {
			sum100 += rets1[k]
		}
		rets100[i] = sum100
	}
	// -----------------------------------

	if len(sb.Data) < numSignals {
		sb.Data = make([][]float64, numSignals)
	}
	for s := 0; s < numSignals; s++ {
		if cap(sb.Data[s]) < n {
			sb.Data[s] = make([]float64, n)
		}
		sb.Data[s] = sb.Data[s][:n]
	}

	for _, g := range gens {
		g.Reset()
	}

	for sIdx, gen := range gens {
		out := sb.Data[sIdx]
		gen.Update(mathCtx, out)
	}

	dayStats := make([]DailyStats, numSignals)
	for sIdx := 0; sIdx < numSignals; sIdx++ {
		sigs := sb.Data[sIdx][:nRet]

		// Z-Score Normalization
		var sum, sumSq float64
		for _, v := range sigs {
			sum += v
			sumSq += v * v
		}
		mean := sum / float64(len(sigs))
		variance := (sumSq / float64(len(sigs))) - mean*mean
		stdInv := 0.0
		if variance > 1e-18 {
			stdInv = 1.0 / math.Sqrt(variance)
		}
		for i := range sigs {
			v := (sigs[i] - mean) * stdInv
			// Soft clamp +/- 5
			if v > 5.0 {
				v = 5.0 * math.Tanh(v/5.0)
			} else if v < -5.0 {
				v = 5.0 * math.Tanh(v/5.0)
			}
			sigs[i] = v
		}

		dayStats[sIdx].Moments = CalcMomentsVectors(sigs, rets1, rets10, rets100)
		dayStats[sIdx].BucketResults = ComputeQuantilesStrided(sigs, rets1, numBuckets, stride, ws)
	}

	return &TaskResult{DayIndex: taskIdx, Stats: dayStats}
}

func printPhysicsWinners(all []SignalSummary) {
	if len(all) == 0 {
		return
	}
	// Sort by Short-Term Prediction (IC_1)
	sort.Slice(all, func(i, j int) bool { return math.Abs(all[i].IC_1) > math.Abs(all[j].IC_1) })

	fmt.Println("\n=== TOP SIGNALS BY PREDICTIVE POWER (IC_1) ===")
	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)
	fmt.Fprintln(w, "RANK\tSYMBOL\tSIGNAL\tIC(1)\tIC(100)\tAUTO\tFLIP%")
	fmt.Fprintln(w, "----\t------\t------\t-----\t-------\t----\t-----")
	for i := 0; i < len(all) && i < 10; i++ {
		s := all[i]
		fmt.Fprintf(w, "#%d\t%s\t%s\t%.4f\t%.4f\t%.3f\t%.1f%%\n",
			i+1, s.Symbol, shortName(s.SignalName), s.IC_1, s.IC_100, s.AutoCorr, s.FlipRate*100)
	}
	w.Flush()
}
```

// --- End File: test.go ---

